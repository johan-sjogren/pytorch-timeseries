{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61d6806-1e63-4454-94c9-17e640d72298",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import contex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85904515-b95b-47cf-9056-afc151f6a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torchdata.datapipes import functional_datapipe\n",
    "import torchdata.datapipes.iter as pipes\n",
    "# import torchdata.datapipes as dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f804040-fd36-4056-8ee4-114a3ccee633",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functional_datapipe(\"parse_pandas_dataframe\")\n",
    "class PandasParserIterDataPipe(pipes.IterDataPipe):\n",
    "    def __init__(self, df) -> None:\n",
    "        self.source_df = df\n",
    "\n",
    "    def __iter__(self):\n",
    "         for row in self.source_df.iterrows():\n",
    "            yield list(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa867a4-1c6c-454b-b53c-a0155c2a7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functional_datapipe('rolling_groupby')\n",
    "class RollingGrouperIterDataPipe(pipes.IterDataPipe):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 datapipe: pipes.IterDataPipe,\n",
    "                 group_key_fn: Callable,\n",
    "                 *,\n",
    "                 window_size=1,\n",
    "                 step_size=1,\n",
    "                 buffer_size: int = 10000,\n",
    "                 # group_size: Optional[int] = None,\n",
    "                 # guaranteed_group_size: Optional[int] = None,\n",
    "                 # drop_remaining: bool = False\n",
    "                ):\n",
    "        # check_lambda_fn(group_key_fn)\n",
    "        self.datapipe = datapipe\n",
    "        self.group_key_fn = group_key_fn\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        self.group_size = window_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.guaranteed_group_size = None\n",
    "        if self.group_size is not None and buffer_size is not None:\n",
    "            assert 0 < self.group_size <= buffer_size\n",
    "            self.guaranteed_group_size = self.group_size\n",
    "        #if guaranteed_group_size is not None:\n",
    "        #    assert group_size is not None and 0 < guaranteed_group_size <= group_size\n",
    "        #    self.guaranteed_group_size = guaranteed_group_size\n",
    "        self.drop_remaining = True\n",
    "        # self.wrapper_class = DataChunk\n",
    "\n",
    "    def _remove_biggest_key(self, buffer_elements, buffer_size):\n",
    "        biggest_key = None\n",
    "        biggest_size = 0\n",
    "        result_to_yield = None\n",
    "        for findkey in buffer_elements.keys():\n",
    "            if len(buffer_elements[findkey]) > biggest_size:\n",
    "                biggest_size = len(buffer_elements[findkey])\n",
    "                biggest_key = findkey\n",
    "\n",
    "        if self.guaranteed_group_size is not None and biggest_size < self.guaranteed_group_size and not self.drop_remaining:\n",
    "            raise RuntimeError('Failed to group items', str(buffer_elements[biggest_key]))\n",
    "\n",
    "        if self.guaranteed_group_size is None or biggest_size >= self.guaranteed_group_size:\n",
    "            result_to_yield = buffer_elements[biggest_key]\n",
    "\n",
    "        new_buffer_size = buffer_size - biggest_size\n",
    "        del buffer_elements[biggest_key]\n",
    "\n",
    "        return result_to_yield, new_buffer_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer_elements: DefaultDict[Any, List] = defaultdict(list)\n",
    "        buffer_size = 0\n",
    "        for x in self.datapipe:\n",
    "            key = self.group_key_fn(x)\n",
    "\n",
    "            buffer_elements[key].append(x)\n",
    "            buffer_size += 1\n",
    "\n",
    "            if self.group_size is not None and self.group_size == len(buffer_elements[key]):\n",
    "                #yield self.wrapper_class(buffer_elements[key])\n",
    "                yield buffer_elements[key]\n",
    "                if self.step_size < self.window_size:\n",
    "                    del buffer_elements[key][:self.step_size]\n",
    "                    buffer_size -= self.step_size\n",
    "                else:\n",
    "                    del buffer_elements[key]\n",
    "                    buffer_size -= self.window_size\n",
    "\n",
    "            if buffer_size == self.buffer_size:\n",
    "                (result_to_yield, buffer_size) = self._remove_biggest_key(buffer_elements, buffer_size)\n",
    "                if result_to_yield is not None:\n",
    "                    yield result_to_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc2dec8b-7839-493d-89b0-ab334f98b3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_idx</th>\n",
       "      <th>volume</th>\n",
       "      <th>agency</th>\n",
       "      <th>sku</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>52.272000</td>\n",
       "      <td>Agency_22</td>\n",
       "      <td>SKU_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3324.269700</td>\n",
       "      <td>Agency_32</td>\n",
       "      <td>SKU_04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>110.700000</td>\n",
       "      <td>Agency_22</td>\n",
       "      <td>SKU_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Agency_58</td>\n",
       "      <td>SKU_23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>28.320000</td>\n",
       "      <td>Agency_48</td>\n",
       "      <td>SKU_07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>238.538700</td>\n",
       "      <td>Agency_22</td>\n",
       "      <td>SKU_05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Agency_58</td>\n",
       "      <td>SKU_17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>126.360000</td>\n",
       "      <td>Agency_31</td>\n",
       "      <td>SKU_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>475.790396</td>\n",
       "      <td>Agency_48</td>\n",
       "      <td>SKU_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1.150200</td>\n",
       "      <td>Agency_40</td>\n",
       "      <td>SKU_04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_idx       volume     agency     sku\n",
       "0         0    52.272000  Agency_22  SKU_01\n",
       "1         0  3324.269700  Agency_32  SKU_04\n",
       "2         0   110.700000  Agency_22  SKU_02\n",
       "3         0     0.000000  Agency_58  SKU_23\n",
       "4         0    28.320000  Agency_48  SKU_07\n",
       "5         0   238.538700  Agency_22  SKU_05\n",
       "6         0     0.000000  Agency_58  SKU_17\n",
       "7         0   126.360000  Agency_31  SKU_01\n",
       "8         0   475.790396  Agency_48  SKU_02\n",
       "9         0     1.150200  Agency_40  SKU_04"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../data/stallion.parquet\")\n",
    "# add time index\n",
    "df[\"time_idx\"] = df[\"date\"].dt.year * 12 + df[\"date\"].dt.month\n",
    "df[\"time_idx\"] -= df[\"time_idx\"].min()\n",
    "\n",
    "# add additional features\n",
    "df[\"month\"] = df.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "df[\"log_volume\"] = np.log(df.volume + 1e-8)\n",
    "df[\"avg_volume_by_sku\"] = df.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n",
    "df[\"avg_volume_by_agency\"] = df.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")\n",
    "time_idx=\"time_idx\"\n",
    "target=\"volume\"\n",
    "group_ids=[\"agency\", \"sku\"]\n",
    "df = df.sort_values(by=\"time_idx\")\n",
    "df = df[[time_idx, target] + group_ids]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f28df738-a2e5-4d76-a461-1b579d93f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[(df.agency == 'Agency_04') & (df.sku == 'SKU_02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d613857-001a-4eae-a832-0aee9feea0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 52.272, 'Agency_22', 'SKU_01']\n"
     ]
    }
   ],
   "source": [
    "datapipe = PandasParserIterDataPipe(df)\n",
    "for x in datapipe:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7cc9af9f-ab43-4343-b355-4534afab25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datapipe.rolling_groupby(group_key_fn=lambda x: x[2] + x[3], window_size=2, step_size=2).batch(2)\n",
    "#ds  = (pipes.FileOpener(datapipe, mode='rt').parse_csv(delimiter=',', skip_lines=1)\n",
    "#            .map(parse_price)\n",
    "#            .rolling(window_size=5, step=1)\n",
    "#            .batch(4)\n",
    "#      )\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a1fd5e2-145b-4f4d-8b94-2bbf8a13d5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[[2, 15.228, 'Agency_04', 'SKU_02'], [3, 5.6160000000000005, 'Agency_04', 'SKU_02']], [[0, 8.6904, 'Agency_28', 'SKU_08'], [1, 7.3485, 'Agency_28', 'SKU_08'], [2, 6.5178, 'Agency_28', 'SKU_08'], [3, 7.667999999999999, 'Agency_28', 'SKU_08']]]\n",
      "\n",
      "[[[2, 3760.02, 'Agency_51', 'SKU_02'], [3, 4886.568, 'Agency_51', 'SKU_02']], [[0, 2653.1279999999997, 'Agency_49', 'SKU_05'], [1, 2614.149, 'Agency_49', 'SKU_05'], [2, 2758.94625, 'Agency_49', 'SKU_05'], [3, 3387.8505, 'Agency_49', 'SKU_05']]]\n",
      "\n",
      "[[[2, 0.0846, 'Agency_29', 'SKU_03'], [3, 0.0, 'Agency_29', 'SKU_03']], [[0, 2211.3873, 'Agency_51', 'SKU_05'], [1, 2227.6815, 'Agency_51', 'SKU_05'], [2, 2267.74725, 'Agency_51', 'SKU_05'], [3, 2991.9255, 'Agency_51', 'SKU_05']]]\n",
      "\n",
      "[[[2, 0.1692, 'Agency_28', 'SKU_03'], [3, 0.6768, 'Agency_28', 'SKU_03']], [[0, 3384.9359999999997, 'Agency_51', 'SKU_01'], [1, 3665.0879999999997, 'Agency_51', 'SKU_01'], [2, 3868.452, 'Agency_51', 'SKU_01'], [3, 5005.8, 'Agency_51', 'SKU_01']]]\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(ds):\n",
    "    print()\n",
    "    print(x)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d910a145-6c56-4c1e-a6a5-218990968db4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The library 'torcharrow' is necessary for this DataPipe but it is not available.Please visit https://github.com/facebookresearch/torcharrow/ to install it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m source_dp \u001b[38;5;241m=\u001b[39m FileLister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#parquet_df_dp = source_dp.load_parquet_as_df(dtype=DTYPE)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m parquet_df_dp \u001b[38;5;241m=\u001b[39m \u001b[43msource_dp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_parquet_as_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mlist\u001b[39m(parquet_df_dp)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-ts/lib/python3.9/site-packages/torch/utils/data/dataset.py:312\u001b[0m, in \u001b[0;36mIterDataPipe.register_datapipe_as_function.<locals>.class_function\u001b[0;34m(cls, enable_df_api_tracing, source_dp, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclass_function\u001b[39m(\u001b[38;5;28mcls\u001b[39m, enable_df_api_tracing, source_dp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     result_pipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource_dp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result_pipe, IterDataPipe):\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m enable_df_api_tracing \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source_dp, DFIterDataPipe):\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-ts/lib/python3.9/site-packages/torchdata/datapipes/iter/util/dataframemaker.py:107\u001b[0m, in \u001b[0;36mParquetDFLoaderIterDataPipe.__init__\u001b[0;34m(self, source_dp, dtype, columns, device, use_threads)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    100\u001b[0m     source_dp: IterDataPipe[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m     use_threads: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m ):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torcharrow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe library \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorcharrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is necessary for this DataPipe but it is not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease visit https://github.com/facebookresearch/torcharrow/ to install it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parquet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe library \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is necessary for this DataPipe but it is not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: The library 'torcharrow' is necessary for this DataPipe but it is not available.Please visit https://github.com/facebookresearch/torcharrow/ to install it."
     ]
    }
   ],
   "source": [
    "from torchdata.datapipes.iter import FileLister\n",
    "# import torcharrow.dtypes as dt\n",
    "# DTYPE = dt.Struct([dt.Field(\"Values\", dt.int32)])\n",
    "#source_dp = FileLister(\".\", masks=\"df*.parquet\")\n",
    "#parquet_df_dp = source_dp.load_parquet_as_df(dtype=DTYPE)\n",
    "#arquet_df_dp = source_dp.load_parquet_as_df()\n",
    "# list(parquet_df_dp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435c744-754e-4e45-88ed-6ef6fce480ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapipe = pipes.IterableWrapper([\"HistoricalQuotes.csv\"])\n",
    "csv = pipes.FileOpener(datapipe, mode='rt').parse_csv(delimiter=',', skip_lines=1)\n",
    "\n",
    "\n",
    "next(iter(csv))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a01260d-d104-443a-9c49-79d7f160fbc9",
   "metadata": {},
   "source": [
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
