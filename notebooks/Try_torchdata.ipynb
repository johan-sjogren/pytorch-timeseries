{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61d6806-1e63-4454-94c9-17e640d72298",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import contex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "85904515-b95b-47cf-9056-afc151f6a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from torchdata.datapipes import functional_datapipe\n",
    "import torchdata.datapipes.iter as pipes\n",
    "import torch\n",
    "# import torchdata.datapipes as dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "effd1594-2e53-42f3-b0b7-2256fb291a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functional_datapipe(\"rolling\")\n",
    "class RollingWindow(pipes.IterDataPipe):\n",
    "    \"\"\"From https://github.com/tcapelle/torchdata/blob/main/02_Custom_timeseries_datapipe.ipynb \"\"\" \n",
    "\n",
    "    def __init__(self, source_dp: pipes.IterDataPipe, window_size, step=1) -> None:\n",
    "        super().__init__()\n",
    "        self.source_dp = source_dp\n",
    "        self.window_size = window_size\n",
    "        self.step = step\n",
    "    \n",
    "    def __iter__(self):\n",
    "        it = iter(self.source_dp)\n",
    "        cur = []\n",
    "        while True:\n",
    "            try:\n",
    "                while len(cur) < self.window_size:\n",
    "                    cur.append(next(it))\n",
    "                yield np.array(cur)\n",
    "                for _ in range(self.step):\n",
    "                    if cur:\n",
    "                        cur.pop(0)\n",
    "                    else:\n",
    "                        next(it)\n",
    "            except StopIteration:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f804040-fd36-4056-8ee4-114a3ccee633",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functional_datapipe(\"parse_pandas_dataframe\")\n",
    "class PandasParserIterDataPipe(pipes.IterDataPipe):\n",
    "    def __init__(self, df) -> None:\n",
    "        self.source_df = df\n",
    "\n",
    "    def __iter__(self):\n",
    "         for row in self.source_df.iterrows():\n",
    "            yield list(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa867a4-1c6c-454b-b53c-a0155c2a7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functional_datapipe('rolling_groupby')\n",
    "class RollingGrouperIterDataPipe(pipes.IterDataPipe):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 datapipe: pipes.IterDataPipe,\n",
    "                 group_key_fn: Callable,\n",
    "                 *,\n",
    "                 window_size=1,\n",
    "                 step_size=1,\n",
    "                 buffer_size: int = 10000,\n",
    "                 # group_size: Optional[int] = None,\n",
    "                 # guaranteed_group_size: Optional[int] = None,\n",
    "                 # drop_remaining: bool = False\n",
    "                ):\n",
    "        # check_lambda_fn(group_key_fn)\n",
    "        self.datapipe = datapipe\n",
    "        self.group_key_fn = group_key_fn\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        self.group_size = window_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.guaranteed_group_size = None\n",
    "        if self.group_size is not None and buffer_size is not None:\n",
    "            assert 0 < self.group_size <= buffer_size\n",
    "            self.guaranteed_group_size = self.group_size\n",
    "        #if guaranteed_group_size is not None:\n",
    "        #    assert group_size is not None and 0 < guaranteed_group_size <= group_size\n",
    "        #    self.guaranteed_group_size = guaranteed_group_size\n",
    "        self.drop_remaining = True\n",
    "        # self.wrapper_class = DataChunk\n",
    "\n",
    "    def _remove_biggest_key(self, buffer_elements, buffer_size):\n",
    "        biggest_key = None\n",
    "        biggest_size = 0\n",
    "        result_to_yield = None\n",
    "        for findkey in buffer_elements.keys():\n",
    "            if len(buffer_elements[findkey]) > biggest_size:\n",
    "                biggest_size = len(buffer_elements[findkey])\n",
    "                biggest_key = findkey\n",
    "\n",
    "        if self.guaranteed_group_size is not None and biggest_size < self.guaranteed_group_size and not self.drop_remaining:\n",
    "            raise RuntimeError('Failed to group items', str(buffer_elements[biggest_key]))\n",
    "\n",
    "        if self.guaranteed_group_size is None or biggest_size >= self.guaranteed_group_size:\n",
    "            result_to_yield = buffer_elements[biggest_key]\n",
    "\n",
    "        new_buffer_size = buffer_size - biggest_size\n",
    "        del buffer_elements[biggest_key]\n",
    "\n",
    "        return result_to_yield, new_buffer_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer_elements: DefaultDict[Any, List] = defaultdict(list)\n",
    "        buffer_size = 0\n",
    "        for x in self.datapipe:\n",
    "            key = self.group_key_fn(x)\n",
    "\n",
    "            buffer_elements[key].append(x)\n",
    "            buffer_size += 1\n",
    "\n",
    "            if self.group_size is not None and self.group_size == len(buffer_elements[key]):\n",
    "                #yield self.wrapper_class(buffer_elements[key])\n",
    "                yield buffer_elements[key]\n",
    "                if self.step_size < self.window_size:\n",
    "                    del buffer_elements[key][:self.step_size]\n",
    "                    buffer_size -= self.step_size\n",
    "                else:\n",
    "                    del buffer_elements[key]\n",
    "                    buffer_size -= self.window_size\n",
    "\n",
    "            if buffer_size == self.buffer_size:\n",
    "                (result_to_yield, buffer_size) = self._remove_biggest_key(buffer_elements, buffer_size)\n",
    "                if result_to_yield is not None:\n",
    "                    yield result_to_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "cc2dec8b-7839-493d-89b0-ab334f98b3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   time_idx       volume     agency     sku\n",
       " 0         0    52.272000  Agency_22  SKU_01\n",
       " 1         0  3324.269700  Agency_32  SKU_04\n",
       " 2         0   110.700000  Agency_22  SKU_02\n",
       " 3         0     0.000000  Agency_58  SKU_23\n",
       " 4         0    28.320000  Agency_48  SKU_07\n",
       " 5         0   238.538700  Agency_22  SKU_05\n",
       " 6         0     0.000000  Agency_58  SKU_17\n",
       " 7         0   126.360000  Agency_31  SKU_01\n",
       " 8         0   475.790396  Agency_48  SKU_02\n",
       " 9         0     1.150200  Agency_40  SKU_04,\n",
       " (21000, 4))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../data/stallion.parquet\")\n",
    "# add time index\n",
    "df[\"time_idx\"] = df[\"date\"].dt.year * 12 + df[\"date\"].dt.month\n",
    "df[\"time_idx\"] -= df[\"time_idx\"].min()\n",
    "\n",
    "# add additional features\n",
    "df[\"month\"] = df.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "df[\"log_volume\"] = np.log(df.volume + 1e-8)\n",
    "df[\"avg_volume_by_sku\"] = df.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n",
    "df[\"avg_volume_by_agency\"] = df.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")\n",
    "time_idx=\"time_idx\"\n",
    "target=\"volume\"\n",
    "group_ids=[\"agency\", \"sku\"]\n",
    "df = df.sort_values(by=\"time_idx\")\n",
    "df = df[[time_idx, target] + group_ids]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(10), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8d613857-001a-4eae-a832-0aee9feea0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 52.272, 'Agency_22', 'SKU_01']\n"
     ]
    }
   ],
   "source": [
    "datapipe = PandasParserIterDataPipe(df)\n",
    "for x in datapipe:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7cc9af9f-ab43-4343-b355-4534afab25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datapipe.rolling_groupby(group_key_fn=lambda x: x[2] + x[3], window_size=1, step_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4a1fd5e2-145b-4f4d-8b94-2bbf8a13d5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0, 52.272, 'Agency_22', 'SKU_01']]\n",
      "\n",
      "[[0, 3324.2697, 'Agency_32', 'SKU_04']]\n",
      "\n",
      "[[0, 110.7, 'Agency_22', 'SKU_02']]\n",
      "\n",
      "[[0, 0.0, 'Agency_58', 'SKU_23']]\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(ds):\n",
    "    print()\n",
    "    print(x)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "893dd1de-0487-49cb-9c1a-8b7c64dcc49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2558461773174316290 -10001110000001011110110110110101101001011110001001110100000010\n",
      "41.2 ns ± 0.199 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "hashed = hash(\"\".join([ str(y) for y in x[0]]))\n",
    "print(hashed, format(hashed, 'b'))\n",
    "%timeit (hashed % 5) > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe294e0b-234c-483d-96b1-3cc1bb82a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6405261876 101111101110010001000101000110100\n",
      "35.5 ns ± 0.745 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "hashed2 = 6405261876\n",
    "print(hashed2, format(hashed2, 'b'))\n",
    "%timeit (hashed2 % 5) > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e20f2d25-59d2-4c7a-ad87-7082d8b44642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640526187 100110001011011010011101101011\n",
      "24.4 ns ± 0.0304 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "hashed2 = 640526187\n",
    "print(hashed2, format(hashed2, 'b'))\n",
    "%timeit (hashed2 % 5) > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d6b9ce4-4597-41c1-b6d7-146ec781ba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 111101\n",
      "24.5 ns ± 0.141 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "hashed2 = 61\n",
    "print(hashed2, format(hashed2, 'b'))\n",
    "%timeit (hashed2 % 5) > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "cc919308-d252-4c19-a0a6-13aa5799e72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johan/miniconda3/envs/pytorch-ts/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:255: UserWarning: Unlimited buffer size is set for `demux`, please be aware of OOM at random places\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def hash_split(x, which_id=0):\n",
    "    hashed = hash(\"\".join([ str(y) for y in x[0]]))\n",
    "    perc = hashed % 100 \n",
    "    if perc < 70:\n",
    "        return 0\n",
    "    if perc < 85:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "train, test, val = ds.demux(num_instances=3, classifier_fn=hash_split, buffer_size=-1)\n",
    "# train, test, val = ds.demux(num_instances=3, classifier_fn=hash_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "307ce588-f99d-4c51-bd9d-862e5c9a65ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14665\n",
      "3160\n",
      "3172\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for i, _ in enumerate(train):\n",
    "        length = i\n",
    "except BufferError:\n",
    "    print(\"Buffer Error\")\n",
    "print(length)\n",
    "\n",
    "try:\n",
    "    for i, _ in enumerate(test):\n",
    "        length = i\n",
    "except BufferError:\n",
    "    print(\"Buffer Error\")\n",
    "print(length)\n",
    "\n",
    "try:\n",
    "    for i, _ in enumerate(val):\n",
    "        length = i\n",
    "except BufferError:\n",
    "    print(\"Buffer Error\")\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0ecb6f73-f021-41a8-bcb8-1e17d1c0f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_split(x, which_id=0):\n",
    "    hashed = hash(\"\".join([ str(y) for y in x[0]]))\n",
    "    perc = hashed % 100 \n",
    "    split_id = 0\n",
    "    if perc >= 70:\n",
    "        split_id =  1\n",
    "    if perc >= 85:\n",
    "        split_id = 2\n",
    "    return split_id == which_id\n",
    "\n",
    "train_split = partial(hash_split, which_id = 0)\n",
    "test_split = partial(hash_split, which_id = 1)\n",
    "val_split = partial(hash_split, which_id = 2)\n",
    "\n",
    "train, test, val = ds.filter(filter_fn=train_split), ds.filter(filter_fn=test_split), ds.filter(filter_fn=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "188ba4e5-4fa7-4d80-b325-02de7fe68d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14665\n",
      "3160\n",
      "3172\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for i, _ in enumerate(train):\n",
    "        length = i\n",
    "except BufferError:\n",
    "    print(\"Buffer Error\")\n",
    "print(length)\n",
    "\n",
    "try:\n",
    "    for i, _ in enumerate(test):\n",
    "        length = i\n",
    "except BufferError:\n",
    "    print(\"Buffer Error\")\n",
    "print(length)\n",
    "\n",
    "try:\n",
    "    for i, _ in enumerate(val):\n",
    "        length = i\n",
    "except BufferError:\n",
    "    print(\"Buffer Error\")\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "da54d221-00c7-4ef8-bbbd-3b293e74beee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 375.1569, 'Agency_18', 'SKU_05'], [1, 386.595, 'Agency_18', 'SKU_05']],\n",
       " [[0, 161.2836, 'Agency_15', 'SKU_05'], [1, 182.8818, 'Agency_15', 'SKU_05']],\n",
       " [[0, 1618.8426, 'Agency_12', 'SKU_05'], [1, 1681.017, 'Agency_12', 'SKU_05']],\n",
       " [[0, 375.1569, 'Agency_18', 'SKU_05'], [1, 386.595, 'Agency_18', 'SKU_05']])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train)), next(iter(test)), next(iter(val)), next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9be457b6-2430-4a80-9b94-f002f9f9be83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-9, 24.4287, 'Agency_25', 'SKU_04'], [-8, 39.1707, 'Agency_25', 'SKU_04']]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train.shuffle()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f547d8b7-027f-431d-96f3-772e8a92d476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.time_idx.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ae61882f-7626-4544-9b8a-ad8fe984ac8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stratified_split(x):\n",
    "    time_idx = x[0][0]\n",
    "    if time_idx < 40:\n",
    "        return 0\n",
    "    if time_idx  < 50:\n",
    "        return 1\n",
    "    return 2\n",
    "    \n",
    "stratified_split(next(iter(ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8dcfbb80-d277-4f98-b939-0ebbfe676bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johan/miniconda3/envs/pytorch-ts/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:255: UserWarning: Unlimited buffer size is set for `demux`, please be aware of OOM at random places\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train, test, val = ds.demux(num_instances=3, classifier_fn=stratified_split, buffer_size=-1)\n",
    "# train, test, val = ds.demux(num_instances=3, classifier_fn=stratified_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9dd99456-a7f4-43c5-8d4a-58eb4e468fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = ds.fork(num_instances=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "59d0d36b-d94d-4371-9290-2edea2878acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6999\n",
      "1749\n",
      "1749\n"
     ]
    }
   ],
   "source": [
    "for i, _ in enumerate(train):\n",
    "    length = i\n",
    "print(length)\n",
    "for i, _ in enumerate(test):\n",
    "    length = i\n",
    "print(length)\n",
    "for i, _ in enumerate(val):\n",
    "    length = i\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "75da1e18-cc96-4de7-8c7e-bbef9cd3c6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 375.1569, 'Agency_18', 'SKU_05'], [1, 386.595, 'Agency_18', 'SKU_05']],\n",
       " [[40, 3081.258, 'Agency_49', 'SKU_04'],\n",
       "  [41, 2362.383, 'Agency_49', 'SKU_04']],\n",
       " [[50, 4.95285, 'Agency_38', 'SKU_14'], [51, 5.2824, 'Agency_38', 'SKU_14']])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train)), next(iter(test)), next(iter(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca021a79-e3cf-4c0f-bc1c-193358273883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groups_\n",
    "\n",
    "grouped = ds.demux(num_instances=3, classifier_fn=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf485c3-2ba7-4913-9174-e84dc0630a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds  = (pipes.FileOpener(datapipe, mode='rt').parse_csv(delimiter=',', skip_lines=1)\n",
    "#            .map(parse_price)\n",
    "#            .rolling(window_size=5, step=1)\n",
    "#            .batch(4)\n",
    "#      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d910a145-6c56-4c1e-a6a5-218990968db4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The library 'torcharrow' is necessary for this DataPipe but it is not available.Please visit https://github.com/facebookresearch/torcharrow/ to install it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m source_dp \u001b[38;5;241m=\u001b[39m FileLister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#parquet_df_dp = source_dp.load_parquet_as_df(dtype=DTYPE)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m parquet_df_dp \u001b[38;5;241m=\u001b[39m \u001b[43msource_dp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_parquet_as_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mlist\u001b[39m(parquet_df_dp)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-ts/lib/python3.9/site-packages/torch/utils/data/dataset.py:312\u001b[0m, in \u001b[0;36mIterDataPipe.register_datapipe_as_function.<locals>.class_function\u001b[0;34m(cls, enable_df_api_tracing, source_dp, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclass_function\u001b[39m(\u001b[38;5;28mcls\u001b[39m, enable_df_api_tracing, source_dp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     result_pipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource_dp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result_pipe, IterDataPipe):\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m enable_df_api_tracing \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source_dp, DFIterDataPipe):\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-ts/lib/python3.9/site-packages/torchdata/datapipes/iter/util/dataframemaker.py:107\u001b[0m, in \u001b[0;36mParquetDFLoaderIterDataPipe.__init__\u001b[0;34m(self, source_dp, dtype, columns, device, use_threads)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    100\u001b[0m     source_dp: IterDataPipe[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m     use_threads: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m ):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torcharrow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe library \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorcharrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is necessary for this DataPipe but it is not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease visit https://github.com/facebookresearch/torcharrow/ to install it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parquet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe library \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is necessary for this DataPipe but it is not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: The library 'torcharrow' is necessary for this DataPipe but it is not available.Please visit https://github.com/facebookresearch/torcharrow/ to install it."
     ]
    }
   ],
   "source": [
    "from torchdata.datapipes.iter import FileLister\n",
    "# import torcharrow.dtypes as dt\n",
    "# dp = pipes.FileLister([camvid_path/\"images\"], masks=\"*.png\")\n",
    "# DTYPE = dt.Struct([dt.Field(\"Values\", dt.int32)])\n",
    "#source_dp = FileLister(\".\", masks=\"df*.parquet\")\n",
    "#parquet_df_dp = source_dp.load_parquet_as_df(dtype=DTYPE)\n",
    "#arquet_df_dp = source_dp.load_parquet_as_df()\n",
    "# list(parquet_df_dp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435c744-754e-4e45-88ed-6ef6fce480ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapipe = pipes.IterableWrapper([\"../data/HistoricalQuotes.csv\"])\n",
    "csv = pipes.FileOpener(datapipe, mode='rt').parse_csv(delimiter=',', skip_lines=1)\n",
    "\n",
    "next(iter(csv))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
